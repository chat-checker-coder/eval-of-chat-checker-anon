{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background Info:\n",
    "1. Data source: downloaded data from https://chateval.org/dbdc5 and extracted under ../datasets/dialogue_breakdowns/\n",
    "2. Data format: https://dbd-challenge.github.io/dbdc3/datasets#format-of-the-json-file \n",
    "3. Evaluation metrics: https://sites.google.com/site/dialoguebreakdowndetection4/evaluation-metrics?authuser=0\n",
    "\n",
    "Note: In the DBDC annotations \"O means not a breakdown, T possible breakdown, and X breakdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chat_checker.breakdown_detection.breakdown_detector import BreakdownIdentifier, GhasselBreakdownIdentifier, OurBreakdownIdentifier\n",
    "from models.benchmark_dialogues import DBDCDialogue\n",
    "from models.configs import BreakdownDetectionConfig\n",
    "from dbdc_eval.reference_evaluator import compute_dbdc_scores as compute_dbdc_scores_reference\n",
    "from dbdc_eval.breakdown_detection_evaluator import compute_dbdc_scores\n",
    "from breakdown_dataset_loader import load_dataset, load_tested_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the challenge, language, and split to evaluate on below\n",
    "challenge = \"dbdc5\"\n",
    "lang = \"ja\"\n",
    "split = \"dev\"\n",
    "\n",
    "if challenge == \"dbdc4\":\n",
    "    assert split == \"eval\"\n",
    "elif challenge == \"dbdc5\":\n",
    "    if lang == \"ja\":\n",
    "        assert split == \"dev\"\n",
    "else:\n",
    "    raise ValueError(f\"Split {split} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_base_dir = Path(f\"./data/{challenge}_{lang}_{split}_subset/\")\n",
    "tested_subset_dir = eval_base_dir / \"annotated_dialogues\"\n",
    "reference_dir = eval_base_dir / \"reference_dialogues\"\n",
    "eval_dir = eval_base_dir / \"eval_files\"\n",
    "os.makedirs(tested_subset_dir, exist_ok=True)\n",
    "os.makedirs(reference_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset Based on the defined split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all json files within the directory\n",
    "dbdc_split_dataset = load_dataset(challenge=challenge, split=split, lang=lang)\n",
    "\n",
    "print(f\"Loaded {len(dbdc_split_dataset)} dialogues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example dialogue:\")\n",
    "print(json.dumps(dbdc_split_dataset[0].model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the subset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify whether to load existing samples and recompute existing annotations\n",
    "load_existing_samples = True\n",
    "recompute_existing_annotations = False\n",
    "# Specify the number of new samples to sample and the maximum number of samples for evaluation\n",
    "n_new_samples = 0\n",
    "max_samples = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_samples: list[DBDCDialogue] = []\n",
    "if load_existing_samples:\n",
    "    tested_samples = load_tested_dialogues(challenge=challenge, split=split, lang=lang)\n",
    "len(tested_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = min(n_new_samples, len(dbdc_split_dataset) - len(tested_samples))\n",
    "new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new samples from the dataset excluding the already tested samples\n",
    "tested_ids = {dialogue.dialogue_id for dialogue in tested_samples}\n",
    "remaining_samples = [dialogue for dialogue in dbdc_split_dataset if dialogue.dialogue_id not in tested_ids]\n",
    "new_samples = random.sample(remaining_samples, n_new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the tested samples before combining with new samples\n",
    "random.shuffle(tested_samples)\n",
    "\n",
    "subset_for_testing = new_samples + tested_samples\n",
    "subset_for_testing = subset_for_testing[:max_samples]\n",
    "len(subset_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([dialogue.dialogue_id for dialogue in subset_for_testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First dialogue from subset for testing:\")\n",
    "print(json.dumps(subset_for_testing[0].model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Evaluation Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the models and breakdown identifiers you want to evaluate on\n",
    "models = {\n",
    "    'gpt-3.5': 'gpt-3.5-turbo-0125',\n",
    "    \"gpt-4o\": \"gpt-4o-2024-08-06\",\n",
    "    # \"gpt-4\": \"gpt-4-0613\"\n",
    "    # \"o3-mini\": \"o3-mini-2025-01-31\",\n",
    "    # \"gpt-4-turbo\": \"gpt-4-turbo-2024-04-09\",\n",
    "    # \"gemini-2.5-pro\": \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "    # \"gemini-2.0-flash\": \"gemini/gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "breakdown_identifiers: dict[str, BreakdownIdentifier] = {\n",
    "    \"ours\": OurBreakdownIdentifier(),\n",
    "    \"ghassel\": GhasselBreakdownIdentifier(),\n",
    "    \"ghassel-taxonomy\": GhasselBreakdownIdentifier(use_breakdown_taxonomy=True),\n",
    "}\n",
    "\n",
    "te_inclusion = {\n",
    "    \"no-tes\": False,\n",
    "    # \"with-tes\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_configs: list[BreakdownDetectionConfig] = []\n",
    "for model_name, model_version in models.items():\n",
    "    for breakdown_identifier_name, breakdown_identifier in breakdown_identifiers.items():\n",
    "        for te_variant, include_te in te_inclusion.items():\n",
    "            config = BreakdownDetectionConfig(\n",
    "                key=f\"{model_name}_{breakdown_identifier_name}_{te_variant}\",\n",
    "                model=model_version,\n",
    "                breakdown_identifier=breakdown_identifier,\n",
    "                include_task_oriented_errors=include_te,\n",
    "            )\n",
    "            eval_configs.append(config)\n",
    "\n",
    "print(f\"Total number of eval configs: {len(eval_configs)}\")\n",
    "print(f\"Config keys:\\n{[config.key for config in eval_configs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Filter the configs to evaluate on a specific subset of configs\n",
    "# allowed_keys = ['gpt-3.5_ghassel-taxonomy_no-tes', 'gpt-4o_ours_no-tes', 'gpt-4o_ghassel_no-tes', 'gpt-4_ghassel_no-tes']\n",
    "# eval_configs = [config for config in eval_configs if config.key in allowed_keys]\n",
    "# print(f\"Filtered to {len(eval_configs)} configs\")\n",
    "# print(f\"Config keys:\\n{[config.key for config in eval_configs]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the breakdown annotations with each config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion_cost\n",
    "\n",
    "from chat_checker.models.dialogue import SpeakerRole\n",
    "from chat_checker.utils.misc_utils import write_prompt_to_txt_file\n",
    "\n",
    "\n",
    "for config in eval_configs:\n",
    "    print(f\"Collecting annotations for {config.key}...\")\n",
    "    config_dir = tested_subset_dir / config.key\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    first_debug_stored = False\n",
    "    for i, dialogue in tqdm(enumerate(subset_for_testing)):\n",
    "        chat_checker_dialogue = dialogue.to_chat_checker_dialogue()\n",
    "        for k, turn in enumerate(chat_checker_dialogue.chat_history):\n",
    "            if turn.role != SpeakerRole.DIALOGUE_SYSTEM:\n",
    "                continue\n",
    "            conversation_history = chat_checker_dialogue.chat_history[:k]\n",
    "            last_bot_utterance = turn.content\n",
    "            has_llm_label = dialogue.turns[k].llm_breakdown_annotations and dialogue.turns[k].llm_breakdown_annotations.get(config.key) is not None\n",
    "            if has_llm_label and not recompute_existing_annotations:\n",
    "                continue\n",
    "            try:\n",
    "                breakdown_info, prompt, model_response = config.breakdown_identifier.identify_breakdowns(\n",
    "                    chat_history=conversation_history,\n",
    "                    last_bot_utterance=last_bot_utterance,\n",
    "                    is_task_oriented=config.include_task_oriented_errors,\n",
    "                    llm_name=config.model,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing dialogue {dialogue.dialogue_id} at turn {k} with config {config}: {e}\")\n",
    "                # We simply skip this turn and continue to the next one (sometimes OpenAI refuses to answer {'refusal': \"I'm sorry, I can't assist with that request.\"})\n",
    "                continue\n",
    "            if not dialogue.turns[k].llm_breakdown_annotations:\n",
    "                dialogue.turns[k].llm_breakdown_annotations = {}\n",
    "            dialogue.turns[k].llm_breakdown_annotations[config.key] = breakdown_info\n",
    "            if k > 0 and not first_debug_stored:\n",
    "                first_debug_stored = True\n",
    "                write_prompt_to_txt_file(prompt, config_dir / \"sample_0_prompt.txt\")\n",
    "                with open(\n",
    "                    config_dir / \"sample_0_model_response.json\", \"w\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    json.dump(model_response.model_dump(), f, ensure_ascii=False, indent=2)\n",
    "                cost = completion_cost(model_response)\n",
    "                with open(\n",
    "                    config_dir / \"sample_0_response_cost.txt\", \"w\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    f.write(f\"Model response cost: {cost:.8f} USD\\n\")\n",
    "            \n",
    "\n",
    "        with open(tested_subset_dir / f\"{dialogue.dialogue_id}.log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dialogue.model_dump(by_alias=True), f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the breakdown annotations against the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the configs to compute evaluation metrics for\n",
    "configs_to_evaluate = [\n",
    "    \"gpt-3.5_ghassel_no-tes\",\n",
    "    \"gpt-3.5_ghassel-taxonomy_no-tes\",\n",
    "    \"gpt-3.5_ours_no-tes\",\n",
    "    \"gpt-4o_ghassel_no-tes\",\n",
    "    \"gpt-4o_ghassel-taxonomy_no-tes\",\n",
    "    \"gpt-4o_ours_no-tes\",\n",
    "    # \"o3-mini_ghassel_no-tes\",\n",
    "    # \"o3-mini_ghassel-taxonomy_no-tes\",\n",
    "    # \"o3-mini_ours_no-tes\",\n",
    "]\n",
    "# configs_to_evaluate = [\n",
    "#     \"gemini-2.0-flash_ours_no-tes\",\n",
    "#     \"gemini-2.0-flash_ghassel_no-tes\",\n",
    "#     \"gemini-2.0-flash_ghassel-taxonomy_no-tes\",\n",
    "# ]\n",
    "# configs_to_evaluate = [\n",
    "#     \"gpt-3.5_ghassel-taxonomy_no-tes\",\n",
    "#     \"gpt-4o_ours_no-tes\",\n",
    "#     \"gpt-4o_ghassel_no-tes\",\n",
    "# ]\n",
    "# configs_to_evaluate = [\n",
    "#     \"gpt-4_ghassel_no-tes\",\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_samples = load_tested_dialogues(challenge=challenge, split=split, lang=lang)\n",
    "len(tested_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from models.benchmark_dialogues import DBDCPredictionsDialogue\n",
    "\n",
    "\n",
    "def compute_reference_scores(config_key: str):\n",
    "    eval_samples: list[DBDCPredictionsDialogue] = []\n",
    "    reference_samples: list[DBDCDialogue] = []\n",
    "    for sample in tested_samples:\n",
    "        try:\n",
    "            eval_sample = sample.to_eval_prediction_dialogue(config_key)\n",
    "            eval_samples.append(eval_sample)\n",
    "            reference_samples.append(sample)\n",
    "        except ValueError:\n",
    "            # We simply skip \n",
    "            continue\n",
    "\n",
    "    # Store the eval json files in a separate directory\n",
    "    # Clear the eval_dir first\n",
    "    if eval_dir.exists():\n",
    "        shutil.rmtree(eval_dir)\n",
    "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sample in eval_samples:\n",
    "        dialogue_id = sample.dialogue_id\n",
    "        with open(eval_dir / f\"{dialogue_id}.labels.json\", \"w\") as f:\n",
    "            json.dump(sample.model_dump(by_alias=True), f, indent=2, ensure_ascii=True)\n",
    "\n",
    "    # Store the reference dialogues in a seperate directory\n",
    "    # Clear the reference_dir first\n",
    "    if reference_dir.exists():\n",
    "        shutil.rmtree(reference_dir)\n",
    "    reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sample in reference_samples:\n",
    "        dialogue_id = sample.dialogue_id\n",
    "        with open(reference_dir / f\"{dialogue_id}.log.json\", \"w\") as f:\n",
    "            json.dump(sample.model_dump(by_alias=True), f, indent=2, ensure_ascii=True)\n",
    "\n",
    "    compute_dbdc_scores_reference(reference_dir.as_posix(), eval_dir.as_posix(), 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for config_key in configs_to_evaluate:\n",
    "    print(f\"Scores for config: {config_key}\")\n",
    "    # print(\"Our Scores\")\n",
    "    res = compute_dbdc_scores(tested_samples, config_key)\n",
    "    res.print_results()\n",
    "    results[config_key] = res\n",
    "\n",
    "    # Uncomment to compare with scores from original eval script\n",
    "    # print(\"\\nOriginal Eval Script Scores:\")\n",
    "    # compute_reference_scores(config_key)\n",
    "\n",
    "    print(\"\\n--------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
