{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chat_checker.breakdown_detection.breakdown_detector import BreakdownIdentifier, OurBreakdownIdentifier\n",
    "from models.benchmark_dialogues import DBDCErrorClassificationDialogue\n",
    "from models.configs import BreakdownDetectionConfig\n",
    "from dbdc_eval.breakdown_detection_analyzer import analyze_error_category_classification_dataset\n",
    "from dbdc_eval.breakdown_classification_evaluator import compute_dbdc_detection_scores, compute_dbdc_error_classification_scores\n",
    "from breakdown_dataset_loader import load_error_classification_dataset, load_tested_error_classification_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Data source: downloaded data from https://chateval.org/dbdc5 and extracted under ../datasets/dialogue_breakdowns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_base_dir = Path(\"./data/dbdc5_error_classification_ja_dev_subset/\")\n",
    "tested_subset_dir = eval_base_dir / \"annotated_dialogues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_error_classification_dataset()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(dataset[0].model_dump(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].to_chat_checker_dialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_error_category_classification_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify whether to load existing samples and recompute existing annotations\n",
    "load_existing_samples = True\n",
    "recompute_existing_annotations = False\n",
    "# Specify the number of new samples to sample and the maximum number of samples for evaluation\n",
    "n_new_samples = 0\n",
    "max_samples = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_samples: list[DBDCErrorClassificationDialogue] = []\n",
    "if load_existing_samples:\n",
    "    tested_samples = load_tested_error_classification_dialogues()\n",
    "len(tested_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = min(n_new_samples, len(dataset) - len(tested_samples))\n",
    "new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new samples from the dataset excluding the already tested samples\n",
    "tested_ids = {dialogue.dialogue_id for dialogue in tested_samples}\n",
    "remaining_samples = [dialogue for dialogue in dataset if dialogue.dialogue_id not in tested_ids]\n",
    "new_samples = random.sample(remaining_samples, n_new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the tested samples before combining with new samples\n",
    "random.shuffle(tested_samples)\n",
    "\n",
    "subset_for_testing = new_samples + tested_samples\n",
    "subset_for_testing = subset_for_testing[:max_samples]\n",
    "len(subset_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([dialogue.dialogue_id for dialogue in subset_for_testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First dialogue from subset for testing:\")\n",
    "print(json.dumps(subset_for_testing[0].model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Evaluation Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # 'gpt-3.5': 'gpt-3.5-turbo-0125',\n",
    "    \"gpt-4o\": \"gpt-4o-2024-08-06\",\n",
    "    # \"o3-mini\": \"o3-mini-2025-01-31\",\n",
    "    # \"gpt-4-turbo\": \"gpt-4-turbo-2024-04-09\",\n",
    "    # \"gemini-2.5-pro\": \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "    # \"gemini-2.0-flash\": \"gemini/gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "breakdown_identifiers: dict[str, BreakdownIdentifier] = {\n",
    "    \"ours\": OurBreakdownIdentifier(),\n",
    "}\n",
    "\n",
    "te_inclusion = {\n",
    "    \"no-tes\": False,\n",
    "    # \"with-tes\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_configs: list[BreakdownDetectionConfig] = []\n",
    "for model_name, model_version in models.items():\n",
    "    for breakdown_identifier_name, breakdown_identifier in breakdown_identifiers.items():\n",
    "        for te_variant, include_te in te_inclusion.items():\n",
    "            config = BreakdownDetectionConfig(\n",
    "                key=f\"{model_name}_{breakdown_identifier_name}_{te_variant}\",\n",
    "                model=model_version,\n",
    "                breakdown_identifier=breakdown_identifier,\n",
    "                include_task_oriented_errors=include_te,\n",
    "            )\n",
    "            eval_configs.append(config)\n",
    "\n",
    "print(f\"Total number of eval configs: {len(eval_configs)}\")\n",
    "print(f\"Config keys:\\n{[config.key for config in eval_configs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the breakdown annotations with each config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion_cost\n",
    "\n",
    "from chat_checker.models.dialogue import SpeakerRole\n",
    "from chat_checker.utils.misc_utils import write_prompt_to_txt_file\n",
    "\n",
    "\n",
    "for config in eval_configs:\n",
    "    config_dir = tested_subset_dir / config.key\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(config_dir)\n",
    "    first_debug_stored = False\n",
    "    for i, dialogue in tqdm(enumerate(subset_for_testing)):\n",
    "        chat_checker_dialogue = dialogue.to_chat_checker_dialogue()\n",
    "        for k, turn in enumerate(chat_checker_dialogue.chat_history):\n",
    "            if turn.role != SpeakerRole.DIALOGUE_SYSTEM:\n",
    "                continue\n",
    "            conversation_history = chat_checker_dialogue.chat_history[:k]\n",
    "            last_bot_utterance = turn.content\n",
    "            has_llm_label = dialogue.turns[k].llm_breakdown_annotations and dialogue.turns[k].llm_breakdown_annotations.get(config.key) is not None\n",
    "            if has_llm_label and not recompute_existing_annotations:\n",
    "                continue\n",
    "            try:\n",
    "                breakdown_info, prompt, model_response = config.breakdown_identifier.identify_breakdowns(\n",
    "                    chat_history=conversation_history,\n",
    "                    last_bot_utterance=last_bot_utterance,\n",
    "                    is_task_oriented=config.include_task_oriented_errors,\n",
    "                    llm_name=config.model,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing dialogue {dialogue.dialogue_id} at turn {k} with config {config}: {e}\")\n",
    "                # We simply skip this turn and continue to the next one (sometimes OpenAI refuses to answer {'refusal': \"I'm sorry, I can't assist with that request.\"})\n",
    "                continue\n",
    "            if not dialogue.turns[k].llm_breakdown_annotations:\n",
    "                dialogue.turns[k].llm_breakdown_annotations = {}\n",
    "            dialogue.turns[k].llm_breakdown_annotations[config.key] = breakdown_info\n",
    "            if k > 0 and not first_debug_stored:\n",
    "                first_debug_stored = True\n",
    "                write_prompt_to_txt_file(prompt, config_dir / \"sample_0_prompt.txt\")\n",
    "                with open(\n",
    "                    config_dir / \"sample_0_model_response.json\", \"w\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    json.dump(model_response.model_dump(), f, ensure_ascii=False, indent=2)\n",
    "                cost = completion_cost(model_response)\n",
    "                with open(\n",
    "                    config_dir / \"sample_0_response_cost.txt\", \"w\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    f.write(f\"Model response cost: {cost:.8f} USD\\n\")\n",
    "            \n",
    "\n",
    "        with open(tested_subset_dir / f\"{dialogue.dialogue_id}.log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dialogue.model_dump(by_alias=True), f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the breakdown annotations against the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown Detection Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in eval_configs:\n",
    "    scores = compute_dbdc_detection_scores(\n",
    "        dialogues=subset_for_testing,\n",
    "        config_key=config.key,\n",
    "        threshold=0.0,\n",
    "    )\n",
    "    scores.print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error category classification scores on agreed breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_category_counts = {}\n",
    "config_mismatch_metrics = {}\n",
    "for config in eval_configs:\n",
    "    scores, category_counts, mismatch_metrics = compute_dbdc_error_classification_scores(\n",
    "        dialogues=subset_for_testing,\n",
    "        config_key=config.key,\n",
    "        mode=\"agreed_breakdowns\",\n",
    "    )\n",
    "    print(scores)\n",
    "    config_category_counts[config.key] = category_counts\n",
    "    config_mismatch_metrics[config.key] = mismatch_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in eval_configs:\n",
    "    print(config_category_counts[config.key])\n",
    "    config_category_counts[config.key].plot_counts(save_dir=tested_subset_dir / config.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in eval_configs:\n",
    "    print(config_mismatch_metrics[config.key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error category classification scores on ground truth breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_category_counts = {}\n",
    "config_mismatch_metrics = {}\n",
    "for config in eval_configs:\n",
    "    scores, category_counts, mismatch_metrics = compute_dbdc_error_classification_scores(\n",
    "        dialogues=subset_for_testing,\n",
    "        config_key=config.key,\n",
    "        mode=\"true_breakdowns\",\n",
    "    )\n",
    "    print(scores)\n",
    "    config_category_counts[config.key] = category_counts\n",
    "    config_mismatch_metrics[config.key] = mismatch_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in eval_configs:\n",
    "    print(config_category_counts[config.key])\n",
    "    config_category_counts[config.key].plot_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in eval_configs:\n",
    "    print(config_mismatch_metrics[config.key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
