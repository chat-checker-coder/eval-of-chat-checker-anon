{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from eval_dataset_loader import get_uss_dataset, get_fed_dial_dataset\n",
    "from models.benchmark_dialogues import EvaluatedDialogue\n",
    "from models.configs import RatingEvalConfig\n",
    "from models.datasets import RatingBenchmarkDataset\n",
    "from rating_evaluation.rating_evaluator import evaluate_ratings\n",
    "from rating_evaluation.dataset_analyzer import plot_human_rating_bar_plots, compute_dataset_statistics\n",
    "from chat_checker.models.chatbot import ChatbotInfo, ChatbotType\n",
    "from chat_checker.dialogue_rating.rating_dimensions import DEFAULT_TASK_ORIENTED_DIMENSIONS, DSTC_CONVERSATIONAL_DIMENSIONS\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.style.use(Path(\"./styles/subfigure.mplstyle\").resolve().as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-Oriented Datasets\n",
    "Data source: Downloaded data from https://chateval.org/sgd and stored under `../datasets/task_oriented_dialogue_systems/uss/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_samples = get_uss_dataset(\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgd_samples[0])\n",
    "print(sgd_samples[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_chatbot_description = \"\"\"A multi-domain chatbot designed for interacting with users across a variety of services and APIs. The chatbot can provide answers and services for requests spanning 20 domains, such as banks, events, media, calendar, travel, and weather.\"\"\"\n",
    "sgd_chatbot = ChatbotInfo(\n",
    "    name=\"Service Chatbot\",\n",
    "    description=sgd_chatbot_description,\n",
    "    type=ChatbotType.TASK_ORIENTED,\n",
    "    interaction_method=\"text-based chat interface\",\n",
    "    task=\"Provide answers and services for requests spanning 20 domains, such as banks, events, media, calendar, travel, and weather.\",\n",
    "    available_languages=[\"English\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_dataset = RatingBenchmarkDataset(\n",
    "    name=\"sgd\",\n",
    "    all_samples=sgd_samples,\n",
    "    rated_samples_path=Path(\"data/sgd/rated_samples.json\"),\n",
    "    chatbot_info=sgd_chatbot,\n",
    "    rating_dimensions=DEFAULT_TASK_ORIENTED_DIMENSIONS,\n",
    "    label_aggregation_method=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MWOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_samples = get_uss_dataset(\"MWOZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mwoz_samples[0])\n",
    "print(mwoz_samples[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_chatbot_description = \"A bot that helps tourists find information about restaurants, hotels, attractions, trains and taxis in Cambridge, UK. The user can also book restaurants, hotels and trains through the bot.\"\n",
    "mwoz_chatbot_task = \"The chatbot should help the user find information about restaurants, hotels, attractions, trains and taxis in Cambridge, UK. The chatbot should also be able to book restaurants, hotels and trains for the user.\"\n",
    "mwoz_chatbot_info = ChatbotInfo(\n",
    "    name=\"Cambridge Tourist Bot\",\n",
    "    description=mwoz_chatbot_description,\n",
    "    type=ChatbotType.TASK_ORIENTED,\n",
    "    interaction_method=\"text-based chat interface\",\n",
    "    task=mwoz_chatbot_task,\n",
    "    available_languages=[\"English\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_dataset = RatingBenchmarkDataset(\n",
    "    name=\"mwoz\",\n",
    "    all_samples=mwoz_samples,\n",
    "    rated_samples_path=Path(\"data/mwoz/rated_samples.json\"),\n",
    "    chatbot_info=mwoz_chatbot_info,\n",
    "    rating_dimensions=DEFAULT_TASK_ORIENTED_DIMENSIONS,\n",
    "    label_aggregation_method=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jddc_samples = get_uss_dataset(\"JDDC\")\n",
    "len(jddc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jddc_samples[0])\n",
    "print(jddc_samples[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jddc_chatbot_description = \"A chatbot for customer service requests for the Chinese E-commerce website Jing Don.\"\n",
    "jddc_chatbot_task = \"The chatbot should help the user with their customer service requests. This includes answering questions about warranty, returns, deliveries, order status, invoices, etc. and managing orders (changing order information, cancelling orders, etc.).\"\n",
    "jddc_chatbot_info = ChatbotInfo(\n",
    "    name=\"E-Commerce Bot\",\n",
    "    description=jddc_chatbot_description,\n",
    "    type=ChatbotType.TASK_ORIENTED,\n",
    "    interaction_method=\"text-based chat interface\",\n",
    "    task=jddc_chatbot_task,\n",
    "    available_languages=[\"Chinese\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jddc_dataset = RatingBenchmarkDataset(\n",
    "    name=\"jddc\",\n",
    "    all_samples=jddc_samples,\n",
    "    rated_samples_path=Path(\"data/jddc/rated_samples.json\"),\n",
    "    chatbot_info=jddc_chatbot_info,\n",
    "    rating_dimensions=DEFAULT_TASK_ORIENTED_DIMENSIONS,\n",
    "    label_aggregation_method=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Datasets\n",
    "Data source: downloaded data from https://chateval.org/dstc10 and extracted to `../datasets/conversational_dialogue_systems/dstc_10_track_5/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_samples = get_fed_dial_dataset()\n",
    "len(fed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_chatbot_description = \"\"\"An open-domain chatbot designed for chit-chat and general conversation. The chatbot can engage in free-form conversation on a wide variety of topics.\"\"\"\n",
    "fed_chatbot_info = ChatbotInfo(\n",
    "    name=\"Open-Domain Chatbot\",\n",
    "    description=fed_chatbot_description,\n",
    "    type=ChatbotType.CONVERSATIONAL,\n",
    "    interaction_method=\"text-based chat interface\",\n",
    "    available_languages=[\"English\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = RatingBenchmarkDataset(\n",
    "    name=\"fed_dial\",\n",
    "    all_samples=fed_samples,\n",
    "    rated_samples_path=Path(\"data/fed_dial/rated_samples.json\"),\n",
    "    chatbot_info=fed_chatbot_info,\n",
    "    rating_dimensions=DSTC_CONVERSATIONAL_DIMENSIONS,\n",
    "    label_aggregation_method=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset to evaluate the dialogue rating on\n",
    "dataset = fed_dataset\n",
    "# Specify whether to load existing samples and recompute existing annotations\n",
    "load_existing_samples = True\n",
    "recompute_existing_ratings = False\n",
    "# Specify the number of new samples to sample and the maximum number of samples for evaluation\n",
    "new_samples = 0\n",
    "max_samples = min(125, len(dataset.all_samples) - (3 + 3))  # Maximum set to 125 to avoid exceeding the maximum number of samples of the FED-Dial dataset\n",
    "print(f\"Max samples: {max_samples}\")\n",
    "\n",
    "print(f\"Using {dataset.name} dataset ({len(dataset.all_samples)} total samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_samples = []\n",
    "if load_existing_samples:\n",
    "    rated_samples = dataset.load_rated_samples()\n",
    "len(rated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_dataset = [sample for sample in dataset.all_samples if sample.dialogue_id not in [s.dialogue_id for s in rated_samples]]\n",
    "remaining_dataset = [sample for sample in remaining_dataset if sample.dialogue_id not in [s.dialogue_id for s in dataset.representative_few_shot_samples]]\n",
    "remaining_dataset = [sample for sample in remaining_dataset if sample.dialogue_id not in [s.dialogue_id for s in dataset.random_few_shot_samples]]\n",
    "fresh_subset = random.sample(remaining_dataset, new_samples)\n",
    "print(f\"Selected {len(fresh_subset)} new samples for rating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_rating = fresh_subset + rated_samples\n",
    "subset_for_rating = subset_for_rating[:max_samples]\n",
    "len(subset_for_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_for_rating[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.representative_few_shot_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = compute_dataset_statistics(dataset.all_samples)\n",
    "with open(dataset.rated_samples_path.parent / \"full_dataset_stats.txt\", \"w\") as f:\n",
    "    f.write(str(stats))\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = compute_dataset_statistics(subset_for_rating)\n",
    "with open(dataset.rated_samples_path.parent / \"subset_dataset_stats.txt\", \"w\") as f:\n",
    "    f.write(str(stats))\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_human_rating_bar_plots(dataset.all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_human_rating_bar_plots(subset_for_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Evaluation Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_checker.dialogue_rating.rating_dimensions import OVERALL_DIMENSION\n",
    "from chat_checker.models.rating import RatingDimension\n",
    "\n",
    "\n",
    "# Configure the models, rating dimensions, few-shot variants, and chatbot info to evaluate on\n",
    "models = {\n",
    "    \"gpt-4o\": \"gpt-4o-2024-08-06\",\n",
    "    # \"o3-mini\": \"o3-mini-2025-01-31\",\n",
    "    # \"gpt-4-turbo\": \"gpt-4-turbo-2024-04-09\"\n",
    "}\n",
    "\n",
    "rating_dimensions: dict[str, list[RatingDimension]] = {\n",
    "    \"only_overall_rd\": [OVERALL_DIMENSION],\n",
    "    \"all_rd\": dataset.rating_dimensions,\n",
    "}\n",
    "# type_specific_dimensions = [d for d in dataset.rating_dimensions if str(d.type) == str(dataset.chatbot_info.type)]\n",
    "# if len(type_specific_dimensions) == 0:\n",
    "#     print(\"WARNING:No type-specific dimensions found\")\n",
    "    \n",
    "# if len(type_specific_dimensions) != 0 and len(type_specific_dimensions + [OVERALL_DIMENSION]) != len(dataset.rating_dimensions):\n",
    "#     print(f\"Adding variant with only type-specific dimensions: {type_specific_dimensions + [OVERALL_DIMENSION]}\")\n",
    "#     rating_dimensions[\"only_type_specific_rd\"] = type_specific_dimensions + [OVERALL_DIMENSION]\n",
    "\n",
    "few_shot_variants: dict[str, list[EvaluatedDialogue]] = {\n",
    "    \"zero_s\": [],\n",
    "    \"random_s\": dataset.random_few_shot_samples,\n",
    "    \"representative_s\": dataset.representative_few_shot_samples,\n",
    "}\n",
    "chatbot_infos: dict[str, ChatbotInfo] = {\n",
    "    \"no_ci\": None,\n",
    "    \"w_ci\": dataset.chatbot_info,\n",
    "}\n",
    "\n",
    "eval_configs: list[RatingEvalConfig] = []\n",
    "for model_name, model_version in models.items():\n",
    "    for rating_key, dimensions in rating_dimensions.items():\n",
    "        for few_shot_key, few_shot_samples in few_shot_variants.items():\n",
    "            for chatbot_key, chatbot_info in chatbot_infos.items():\n",
    "                eval_config = RatingEvalConfig(\n",
    "                    key=f\"{model_name}-{rating_key}-{few_shot_key}-{chatbot_key}\",\n",
    "                    model=model_version,\n",
    "                    rating_dimensions=dimensions,\n",
    "                    few_shot_samples=[d.to_chat_checker_dialogue() for d in few_shot_samples],\n",
    "                    chatbot_info=chatbot_info,\n",
    "                )\n",
    "                eval_configs.append(eval_config)\n",
    "\n",
    "print(f\"Total number of eval configs: {len(eval_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = {}\n",
    "for eval_config in tqdm(eval_configs, desc=\"Computing LLM Ratings\"):\n",
    "    print (f\"Evaluating {eval_config.key}\")\n",
    "\n",
    "    correlations = evaluate_ratings(\n",
    "        subset_for_rating,\n",
    "        config=eval_config,\n",
    "        benchmark_dataset=dataset,\n",
    "        recompute_existing_ratings=recompute_existing_ratings,\n",
    "    )\n",
    "    correlation_results[eval_config.key] = correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory to save the plots\n",
    "plot_dir = dataset.rated_samples_path.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze whether Spearman correlation is bettter for overall rating dimension or for ensembled rating\n",
    "Verdict for FED-Dial: no big difference\n",
    "Verdict for SGD: no big difference  \n",
    "--> use overall dimension for simplicity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store correlations for comparison\n",
    "overall_correlations = []\n",
    "ensemble_correlations = []\n",
    "differences = []\n",
    "\n",
    "for config_key, correlations in correlation_results.items():\n",
    "    overall_corr = correlations[\"spearman_correlation_overall\"]\n",
    "    ensemble_corr = correlations[\"spearman_correlation_dimension_ensemble\"]\n",
    "    \n",
    "    \n",
    "    if overall_corr is not None:\n",
    "        overall_correlations.append(overall_corr)\n",
    "    if ensemble_corr is not None and \"only_overall_rd\" not in config_key:\n",
    "        ensemble_correlations.append(ensemble_corr)\n",
    "    if overall_corr is not None and ensemble_corr is not None:\n",
    "        difference = overall_corr - ensemble_corr\n",
    "        differences.append(difference)\n",
    "\n",
    "print(\"Overall Rating Correlations:\")\n",
    "print(f\"Mean: {np.mean(overall_correlations):.3f}\")\n",
    "print(f\"Median: {np.median(overall_correlations):.3f}\")\n",
    "print(f\"Std: {np.std(overall_correlations):.3f}\")\n",
    "print(f\"Min: {np.min(overall_correlations):.3f}\")\n",
    "print(f\"Max: {np.max(overall_correlations):.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Ensemble Rating Correlations:\") \n",
    "print(f\"Mean: {np.mean(ensemble_correlations):.3f}\")\n",
    "print(f\"Median: {np.median(ensemble_correlations):.3f}\")\n",
    "print(f\"Std: {np.std(ensemble_correlations):.3f}\")\n",
    "print(f\"Min: {np.min(ensemble_correlations):.3f}\")\n",
    "print(f\"Max: {np.max(ensemble_correlations):.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Differences:\")\n",
    "print(f\"Mean: {np.mean(differences):.3f}\")\n",
    "print(f\"Std: {np.std(differences):.3f}\")\n",
    "print(f\"Min: {np.min(differences):.3f}\")\n",
    "print(f\"Max: {np.max(differences):.3f}\")\n",
    "print()\n",
    "\n",
    "# Create box plot to visualize distribution of correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([overall_correlations, ensemble_correlations], labels=['Overall', 'Ensemble'])\n",
    "# plt.title('Distribution of Spearman Correlations: Overall vs Ensemble Ratings')\n",
    "plt.ylabel('Spearman Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plot_name = \"overall_vs_ensemble_correlations\"\n",
    "plt.savefig(plot_dir / f\"{plot_name}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.savefig(plot_dir / f\"{plot_name}.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze which configuration performed the best\n",
    "Verdict for FED-Dial: use few-shot (no big diff between representative/random), other parameters no big diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_config_results = sorted(correlation_results.items(), key=lambda x: x[1][\"spearman_correlation_overall\"], reverse=True)\n",
    "print(\"\\nBest performing configurations by Spearman correlation:\")\n",
    "for config_key, results in sorted_config_results:\n",
    "    correlation = results[\"spearman_correlation_overall\"]\n",
    "    print(f\"{config_key}: {correlation:.3f}\")\n",
    "\n",
    "# Save rankings to text file\n",
    "with open(plot_dir / \"config_rankings.txt\", \"w\") as f:\n",
    "    f.write(\"Configuration Rankings by Spearman Correlation:\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    for config_key, results in sorted_config_results:\n",
    "        correlation = results[\"spearman_correlation_overall\"]\n",
    "        f.write(f\"{config_key}: {correlation:.3f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dimension_names = {\n",
    "    \"only_overall_rd\": \"only overall RD\",\n",
    "    \"only_type_specific_rd\": \"only type-specific RDs\",\n",
    "    \"all_rd\": \"all RDs\",\n",
    "}\n",
    "few_shot_names = {\n",
    "    \"zero_s\": \"zero shot\",\n",
    "    \"random_s\": \"random 3-shot\",\n",
    "    \"representative_s\": \"spectrum 3-shot\",\n",
    "}\n",
    "chatbot_info_names = {\n",
    "    \"no_ci\": \"no CI\",\n",
    "    \"w_ci\": \"with CI\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_and_save_correlation_plot(df: pd.DataFrame, x_axis: str, plot_name: str):\n",
    "    sns.barplot(x=x_axis, y='spearman_overall', data=df, color=\"dodgerblue\", width=0.4, capsize=0.05)\n",
    "    plt.ylabel('Average Spearman Correlation')\n",
    "    plt.xlabel(None)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(plot_dir / f\"{plot_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(plot_dir / f\"{plot_name}.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your results into a pandas DataFrame for easier analysis\n",
    "data = []\n",
    "for config_key, results in correlation_results.items():\n",
    "    model_base, model_tag, rd, fs, ci = config_key.split('-')\n",
    "    model = f\"{model_base}-{model_tag}\"\n",
    "    data.append({\n",
    "        'config_key': config_key,\n",
    "        'model': model,\n",
    "        'rating_dimensions': rating_dimension_names.get(rd, rd),\n",
    "        'few_shot_samples': few_shot_names.get(fs, fs),\n",
    "        'chatbot_info': chatbot_info_names.get(ci, ci),\n",
    "        'spearman_overall': results['spearman_correlation_overall']\n",
    "    })\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "print(\"\\nDataFrame of Correlation Results:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Calculate Baseline Performance\n",
    "baseline_config = df[(df['rating_dimensions'] == rating_dimension_names.get('only_overall_rd')) &\n",
    "                    (df['few_shot_samples'] == few_shot_names.get('zero_s')) &\n",
    "                    (df['chatbot_info'] == chatbot_info_names.get('no_ci'))]\n",
    "baseline_correlation = baseline_config['spearman_overall'].iloc[0] if not baseline_config.empty else None\n",
    "print(f\"\\nBaseline Configuration (gpt-4o-only_overall_rd-zero_s-no_ci) Spearman Correlation: {baseline_correlation:.3f}\" if baseline_correlation is not None else \"\\nBaseline Configuration not found.\")\n",
    "\n",
    "# 3. Analyze the Impact of Each Factor Individually\n",
    "print(\"\\n--- Impact of Rating Dimensions ---\")\n",
    "print(df.groupby('rating_dimensions')['spearman_overall'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n--- Impact of Few-Shot Samples ---\")\n",
    "print(df.groupby('few_shot_samples')['spearman_overall'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n--- Impact of Chatbot Info ---\")\n",
    "print(df.groupby('chatbot_info')['spearman_overall'].mean().sort_values(ascending=False))\n",
    "\n",
    "# 4. Visualization\n",
    "\n",
    "print(\"\\n--- Visualizations ---\")\n",
    "\n",
    "# Bar plot of average Spearman correlation by Rating Dimensions\n",
    "create_and_save_correlation_plot(df, 'rating_dimensions', 'average_spearman_correlation_by_rating_dimensions')\n",
    "\n",
    "# Bar plot of average Spearman correlation by Few-Shot Samples\n",
    "create_and_save_correlation_plot(df, 'few_shot_samples', \"average_spearman_correlation_by_few_shot_samples\")\n",
    "\n",
    "# Bar plot of average Spearman correlation by Chatbot Info\n",
    "create_and_save_correlation_plot(df, \"chatbot_info\", \"average_spearman_correlation_by_chatbot_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
